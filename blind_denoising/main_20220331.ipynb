{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461df175-8ebb-4901-8e2d-d37bae34533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torchsummary in /root/miniconda3/lib/python3.8/site-packages (1.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msakura_1986\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install torchsummary\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "import h5py\n",
    "import numpy as np\n",
    "import megengine.data as data\n",
    "import megengine.data.transform as T\n",
    "# import megengine\n",
    "import cv2\n",
    "import os\n",
    "from megengine.data.dataset import Dataset\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "from scipy import io\n",
    "\n",
    "# import megengine as mge\n",
    "# import megengine.module as M\n",
    "# import megengine.functional as F\n",
    "\n",
    "# from megengine.data.transform import ToMode\n",
    "# from megengine.data import DataLoader, RandomSampler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "# import torch.nn as M\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc8443-298c-49ab-96c1-274f3c8ac075",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10177e4e-211b-40bb-b184-8090c4dc3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model = 'Predictor_l'\n",
    "    dataset = 'MEG'\n",
    "    seed = 42\n",
    "    Meg_train_length = 7000    #0-8192\n",
    "    folder = \"/root/autodl-tmp/SIDD_Small_Raw_Only/\"\n",
    "    bs = 64 # batch size\n",
    "    cut_size = 256\n",
    "    device = 'cuda:0'\n",
    "    load_pretrain = False\n",
    "    lr = 1e-3\n",
    "    n_steps = 10\n",
    "    gamma = 0.7\n",
    "    start_epoch = 0\n",
    "    n_epochs = 200\n",
    "    model_name = model+'_'+dataset+'_'+\"nkh_train00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04195e-ab77-410e-bcec-772a579fe696",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a208be-ae1e-4599-b011-0196d23ddb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(img1, img2):\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "#     mse = torch.mean((img1 - img2) ** 2)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f69b3b-0a15-4a3f-95b9-81683558e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTensor(img):\n",
    "    img = torch.from_numpy(img)\n",
    "    return img.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee8ec37-3a58-476f-bea8-f1406f61b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_str(cls):\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "    cls.__str__ = __str__\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2f97e6-4f42-4767-a85b-5fcacb1aa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "setup_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31df4c78-33bb-4354-836a-8636ba1c1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(list1):\n",
    "    return sum(list1)/len(list1)\n",
    "\n",
    "def compute_score(igt, ipred):\n",
    "    pred_name, gt_name = sys.argv[1:3]\n",
    "    samples_gt = igt\n",
    "    samples_pred = ipred\n",
    "    means = samples_gt.mean(axis=(1, 2))\n",
    "    weight = (1/means)**0.5\n",
    "    diff = torch.abs(samples_pred - samples_gt).mean(axis=(1, 2))\n",
    "    diff = diff * weight\n",
    "    score = diff.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ace513-543c-4208-9e65-8e18168d9c97",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd448fe-5b64-4c14-8c4e-16d5f419f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.input_path = data_path +  '/competition_train_input.0.2.bin'\n",
    "        self.gt_path = data_path + '/competition_train_gt.0.2.bin'\n",
    "        self.content = open(self.input_path, 'rb').read()\n",
    "        self.input_images = np.frombuffer(self.content, dtype = 'uint16').reshape((-1,256,256))\n",
    "        self.content = open(self.gt_path, 'rb').read()\n",
    "        self.gt_images = np.frombuffer(self.content, dtype = 'uint16').reshape((-1,256,256))\n",
    "        print(len(self.input_images))\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inoisy = np.float32([self.input_images[idx, :, :]]) * np.float32(1 / 65536)\n",
    "        igt = np.float32([self.gt_images[idx, :, :]]) * np.float32(1 / 65536)\n",
    "        return toTensor(inoisy), toTensor(igt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5379bd8-e07c-4cb0-b298-1207fc573def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIDDDataset(Dataset):\n",
    "    def __init__(self, mode, data_list):\n",
    "        self.image_folder = cfg.folder + \"new_Data/\"\n",
    "        self.image_list_all = os.listdir(self.image_folder)\n",
    "        if mode == 1:  #train or val\n",
    "            data_list_str = ' '.join(data_list)  #全部训练集源文件名组成的字符串\n",
    "            self.image_list = [img_idx for img_idx in self.image_list_all if img_idx.rpartition('_')[0] in data_list_str]\n",
    "        else:\n",
    "            self.image_list = self.image_list_all\n",
    "        self.cut = cfg.cut_size\n",
    "\n",
    "    # get the sample\n",
    "    def __getitem__(self, idx):\n",
    "        # get the index\n",
    "        image_id = self.image_list[idx]\n",
    "        inoisy = scipy.io.loadmat(self.image_folder + self.image_list[idx] + \"/GT_RAW_010.MAT\")\n",
    "        igt = scipy.io.loadmat(self.image_folder + self.image_list[idx] + \"/NOISY_RAW_010.MAT\")\n",
    "        inoisy = np.float32([np.array(inoisy['var_name'])])\n",
    "        igt = np.float32([np.array(igt['var_name'])])\n",
    "        return toTensor(inoisy), toTensor(igt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194f9eed-c265-495a-83b2-d8b7c287ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "# a = os.listdir(\"/root/autodl-tmp/SIDD_Small_Raw_Only/new_Data/\")\n",
    "# len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c3555b-9ea3-4918-a4c4-772c382bffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "if cfg.dataset==\"MEG\":\n",
    "    Meg_data_path = '/root/data/burst_raw/'\n",
    "    dataset = MEGDataset(Meg_data_path)\n",
    "    train_set, vali_set = random_split(dataset=dataset, lengths=[cfg.Meg_train_length, 8192-cfg.Meg_train_length])\n",
    "elif cfg.dataset == 'SIDD':\n",
    "    random.seed(32)\n",
    "    ori_list = os.listdir(\"/root/autodl-tmp/SIDD_Small_Raw_Only/Data/\")\n",
    "    random.shuffle(ori_list)\n",
    "    train_list = ori_list[:138]    #0-160，需要根据val_list中的数据数设定计算psnr时的被除数\n",
    "    val_list = ori_list[138:]\n",
    "    train_set = SIDDDataset(1, train_list)\n",
    "    vali_set = SIDDDataset(1, val_list)\n",
    "    \n",
    "else:\n",
    "    print('set an existing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409d944c-2d94-4d42-b65e-77018ca1f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Data.DataLoader(train_set,\n",
    "      shuffle=True, batch_size=cfg.bs\n",
    ")\n",
    "vali_dataloader = Data.DataLoader(vali_set,\n",
    "      shuffle=False, batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c9476-c817-4255-a280-1bf183e0dbb9",
   "metadata": {},
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ae0707-36bc-42b2-809e-4e71a26589f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 50, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(50, 50, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 50, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(50, 50, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(50, 50, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(50, 4, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.reshape((n, c, h // 2, 2, w // 2, 2)).permute(0, 1, 3, 5, 2, 4).reshape((n, c * 4, h // 2, w // 2))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape((n, c, 2, 2, h // 2, w // 2)).permute(0, 1, 4, 2, 5, 3).reshape((n, c, h, w))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e7c0b9-2a2b-47f4-82cf-57bb2ba99541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor_l(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 100, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(100, 200, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(200, 200, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(200, 200, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(200, 100, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "            nn.Conv2d(100, 4, 3, padding = 1, bias = True),\n",
    "            nn.LeakyReLU(negative_slope = 0.125),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.reshape((n, c, h // 2, 2, w // 2, 2)).permute(0, 1, 3, 5, 2, 4).reshape((n, c * 4, h // 2, w // 2))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape((n, c, 2, 2, h // 2, w // 2)).permute(0, 1, 4, 2, 5, 3).reshape((n, c, h, w))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "706c124d-eec8-4da0-b6de-e0f48dd49a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a9db45e-1ce2-400e-aeb2-b324cd016148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of params: 0.784k\n"
     ]
    }
   ],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "sam = SpatialAttention()\n",
    "total = sum([param.nelement() for param in sam.parameters()])\n",
    "print('  Number of params: %.3fk' % (total / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ea074e-c373-46e4-a007-ee2fbf83cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, 3, stride=3, padding=1),  # (b, 16, 10, 10)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),  # (b, 8, 3, 3)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # (b, 8, 2, 2)\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # (b, 8, 3, 3)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1),  # (b, 8, 2, 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 32, 4, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 5, stride=2, padding=1),  # (b, 8, 15, 15)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=2, padding=1),  # (b, 1, 28, 28)\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.ConvTranspose2d(8, 4, 4, stride=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "#         self.cam = ChannelAttention(self.inplanes)\n",
    "        self.sam = SpatialAttention()\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.reshape((n, c, h // 2, 2, w // 2, 2)).permute(0, 1, 3, 5, 2, 4).reshape((n, c * 4, h // 2, w // 2))\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "        x = self.encoder(x)\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "        x=self.sam(x)\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "        x = self.decoder(x)\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "#         x=self.sam(x)\n",
    "        x = x.reshape((n, c, 2, 2, h // 2, w // 2)).permute(0, 1, 4, 2, 5, 3).reshape((n, c, h, w))\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491ccce-155c-4edd-842c-a6ada9484e80",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8455ac7d-2e39-4603-8052-c95238e6c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of params: 1088.004k\n"
     ]
    }
   ],
   "source": [
    "net = Predictor_l()\n",
    "total = sum([param.nelement() for param in net.parameters()])\n",
    "print('  Number of params: %.3fk' % (total / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96c7ebcc-acca-4b8a-a802-ccc79d715a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 1, 256, 256])\n",
      "y.shape: torch.Size([1, 1, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [1, 100, 128, 128]           3,700\n",
      "         LeakyReLU-2         [1, 100, 128, 128]               0\n",
      "            Conv2d-3         [1, 200, 128, 128]         180,200\n",
      "         LeakyReLU-4         [1, 200, 128, 128]               0\n",
      "            Conv2d-5         [1, 200, 128, 128]         360,200\n",
      "         LeakyReLU-6         [1, 200, 128, 128]               0\n",
      "            Conv2d-7         [1, 200, 128, 128]         360,200\n",
      "         LeakyReLU-8         [1, 200, 128, 128]               0\n",
      "            Conv2d-9         [1, 100, 128, 128]         180,100\n",
      "        LeakyReLU-10         [1, 100, 128, 128]               0\n",
      "           Conv2d-11           [1, 4, 128, 128]           3,604\n",
      "        LeakyReLU-12           [1, 4, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 1,088,004\n",
      "Trainable params: 1,088,004\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 201.00\n",
      "Params size (MB): 4.15\n",
      "Estimated Total Size (MB): 205.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 调试Autoencoder输出维度\n",
    "x = torch.randn(1,1,256,256)\n",
    "y = net(x)\n",
    "print(f'x.shape: {x.shape}')\n",
    "print(f'y.shape: {y.shape}')\n",
    "summary(net,(1,256,256),batch_size=1,device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5aab72-ab68-4b3e-8361-89c204f14e4c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d4b7dcd-ce67-44cc-88f9-6041f568bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(train_loader, vali_loader):\n",
    "    if cfg.model == 'Predictor':\n",
    "        model = Predictor().to(cfg.device)\n",
    "    elif cfg.model == 'Autoencoder':\n",
    "        model=Autoencoder().to(cfg.device)\n",
    "    elif cfg.model == 'Predictor_l':\n",
    "        model = Predictor_l().to(cfg.device)\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    if cfg.load_pretrain:\n",
    "        # TODO\n",
    "        s = 0\n",
    "\n",
    "    criterion_L1 = torch.nn.L1Loss().to(cfg.device)\n",
    "    optimizer = torch.optim.Adam([paras for paras in model.parameters() if paras.requires_grad == True], lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.n_steps, gamma=cfg.gamma)\n",
    "\n",
    "    loss_list = []\n",
    "    \n",
    "    # 开始训练\n",
    "    for idx_epoch in range(cfg.start_epoch, cfg.n_epochs):\n",
    "        # record loss\n",
    "        loss_epoch = []\n",
    "        wandb.log({\"epoch\": idx_epoch})\n",
    "        for idx_iter, (inoisy, igt) in tqdm(enumerate(train_loader)):\n",
    "            inoisy, igt = Variable(inoisy).to(cfg.device), Variable(igt).to(cfg.device)\n",
    "#             print(f'igt.shape: {igt.shape}')\n",
    "            ipred = model(inoisy)\n",
    "#             print(len(ipred))\n",
    "            loss = criterion_L1(ipred, igt)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_epoch.append(loss.data.cpu())\n",
    "            \n",
    "        scheduler.step()\n",
    "        lr_now = scheduler.get_last_lr()\n",
    "        wandb.log({\"lr\": float(lr_now[-1])})\n",
    "        \n",
    "        # 输出结果\n",
    "        if idx_epoch % 1 == 0:\n",
    "            print('Epoch--%4d, loss--%f' %\n",
    "                  (idx_epoch + 1, float(np.array(loss_epoch).mean())))\n",
    "            wandb.log({\"loss\": float(np.array(loss_epoch).mean())})\n",
    "            \n",
    "        # 保存模型\n",
    "        if idx_epoch % 5 == 0:\n",
    "            torch.save({'epoch': idx_epoch + 1, 'state_dict': model.state_dict()},\n",
    "                       'log/' + cfg.model_name + '_' + 'epoch' + str(idx_epoch + 1) + '.pth.tar')\n",
    "            \n",
    "        # validation\n",
    "        if idx_epoch % 5 == 0:\n",
    "            psnr_list = []\n",
    "            score_list = []\n",
    "            for idx_iter, (inoisy, igt) in tqdm(enumerate(vali_loader)):\n",
    "                inoisy, igt = Variable(inoisy).to(cfg.device), Variable(igt).to(cfg.device)\n",
    "                with torch.no_grad():\n",
    "                    ipred = model(inoisy)\n",
    "                s = compute_score(igt, ipred)\n",
    "                p = psnr(inoisy, ipred)\n",
    "                psnr_list.append(p)\n",
    "                score_list.append(s)\n",
    "            val_len = 8192 - cfg.Meg_train_length\n",
    "            score = 5*math.log(100/mean(score_list),10)\n",
    "            print(\"Tested PSNR\", str(sum(psnr_list) / val_len))\n",
    "            print(\"Tested SCORE\", str(score))\n",
    "            wandb.log({\"PSNR\": float(sum(psnr_list) / val_len)})\n",
    "            wandb.log({\"SCORE\": float(score)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e614f4-b41b-4086-a51d-824a6097e1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/work/wandb/run-20220329_214148-3riqd39l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hanhan_tie/blind-denoising/runs/3riqd39l\" target=\"_blank\">Predictor_l_MEG_nkh_train00</a></strong> to <a href=\"https://wandb.ai/hanhan_tie/blind-denoising\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "110it [04:24,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   1, loss--0.016773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [04:06,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested PSNR 37.321229833084466\n",
      "Tested SCORE 17.257641838163263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:52,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   2, loss--0.008505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:28,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   3, loss--0.007546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:45,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   4, loss--0.004978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:14,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   5, loss--0.004087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:07,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   6, loss--0.004217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [03:15,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested PSNR 43.58430601606433\n",
      "Tested SCORE 19.190863110399068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:59,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   7, loss--0.004694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:06,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   8, loss--0.003663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:53,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--   9, loss--0.003768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [05:26,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  10, loss--0.003233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [04:22,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  11, loss--0.002803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [03:35,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested PSNR 46.02188567987224\n",
      "Tested SCORE 20.478745500399466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [04:36,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  12, loss--0.002739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:58,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  13, loss--0.002894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:21,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  14, loss--0.002923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:48,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  15, loss--0.003148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:40,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  16, loss--0.002887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [03:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested PSNR 45.1491887177397\n",
      "Tested SCORE 20.000439127408104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:07,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  17, loss--0.002915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:31,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  18, loss--0.002785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [04:04,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  19, loss--0.002712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:58,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  20, loss--0.002873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [03:22,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  21, loss--0.002604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [03:25,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested PSNR 46.05047670786813\n",
      "Tested SCORE 20.407285780276023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [09:53,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  22, loss--0.002555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [09:32,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  23, loss--0.002633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [10:05,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  24, loss--0.002620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [07:23,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  25, loss--0.002746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [06:33,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch--  26, loss--0.002638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [02:10,  3.17it/s]"
     ]
    }
   ],
   "source": [
    "train_cfg = {\n",
    "                \"batch size\": cfg.bs,\n",
    "                \"cut_size\":cfg.cut_size,\n",
    "                \"num_epoch\":cfg.n_epochs,\n",
    "                \"init_lr\":cfg.lr,\n",
    "                \"n_steps\":cfg.n_steps,\n",
    "                \"change_gamma\":cfg.gamma,\n",
    "                \"Model Name\":cfg.model_name\n",
    "}\n",
    "\n",
    "run_name = cfg.model_name\n",
    "\n",
    "wandb.init(project=\"blind-denoising\",\n",
    "           config = train_cfg,\n",
    "           entity=\"hanhan_tie\",\n",
    "#            entity=\"sakura_1986\",\n",
    "           name = run_name)\n",
    "\n",
    "wandb.run.save()\n",
    "\n",
    "train(train_dataloader, vali_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a8435-c451-4b2d-b157-a6a751c5964b",
   "metadata": {},
   "source": [
    "#  Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3096a1ea-051c-4d86-9c7b-1be558fcc59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Predictor_l().to('cuda:0')\n",
    "model = torch.load('./log/Predictor_l_MEG_nkh_train00_epoch96.pth.tar')\n",
    "net.load_state_dict(model['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0385d595-f5cd-479a-a91b-0989156e2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "print('prediction')\n",
    "\n",
    "content = open('/root/data/burst_raw/competition_test_input.0.2.bin', 'rb').read()\n",
    "samples_ref = np.frombuffer(content, dtype = 'uint16').reshape((-1,256,256))\n",
    "fout = open('/root/out2.bin', 'wb')\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(samples_ref), 64)):\n",
    "    i_end = min(i + 64, len(samples_ref))\n",
    "    batch_inp = torch.tensor(np.float32(samples_ref[i:i_end, None, :, :]) * np.float32(1 / 65536)).to('cuda:0')\n",
    "    pred = net(batch_inp)\n",
    "#     pred = (pred.numpy()[:, 0, :, :] * 65536).clip(0, 65535).astype('uint16')\n",
    "    pred = (pred.detach().cpu().numpy()[:, 0, :, :] * 65536).clip(0, 65535).astype('uint16')\n",
    "    fout.write(pred.tobytes())\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a1d73-2fd5-491e-aaf4-754d32114865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc16ba4-1e2f-4a0d-86ba-42b3ee117185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463923a-6cd0-4ad0-9546-ac5fc577f74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8fa79-0741-4743-a8be-04dbf9050e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35392cd-4583-4ab0-8f11-41d64bfa785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = scipy.io.loadmat(\"/root/autodl-tmp/SIDD_Small_Raw_Only/\" + \"new_Data/\" + \"0001_001_S6_00100_00060_3200_L_0/NOISY_RAW_010.MAT\")\n",
    "# inoisy = np.float32([np.array(data['var_name'])])\n",
    "# print(inoisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc3a30-0258-47fc-a46c-fa492fdcbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8a0e2-2500-4da6-a6a9-f394c7df9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inoisy = h5py.File(\"/root/autodl-tmp/SIDD_Small_Raw_Only/\" + \"Data/\" + \"0001_001_S6_00100_00060_3200_L/NOISY_RAW_010.MAT\")\n",
    "# inoisy = np.float32([np.array(inoisy['x'])])\n",
    "# print(inoisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78dd47-2474-4f30-a9d5-8c458a019228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f67e23-968e-4cd1-b397-0cdbd68e3c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6c001-dc4c-4c22-b443-c92bed8f63e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697067f5-7bdc-43ea-9c0f-b63ce4a9b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9be1e-b791-4035-a235-467c46f4afa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6671f5-2eb5-49d2-9c1d-ecdb3b12f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a5284-da44-44f2-9839-a013064cbdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a88f31-dee2-4c27-8f19-34c1f5f1a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764001d-75b2-4dd1-a4b5-9d033d8c2773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca99ce6-82d3-4b2c-98fc-4a18814c2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Autoencoder(M.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.encoder = M.Sequential([\n",
    "#             # Input(shape=(28, 28, 1,)),\n",
    "#             M.Conv2D(4, 50, 3, padding = 1, bias = True),\n",
    "#             M.MaxPooling2D((2, 2), padding='same'),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(3,3)),\n",
    "#             M.MaxPooling2D((1, 1), padding='same'),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(4,4)),\n",
    "#             M.MaxPooling2D((1, 1), padding='same'),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(4,4)),\n",
    "#             M.MaxPooling2D((2, 2), padding='same'),\n",
    "#         ])\n",
    "#         self.decoder=M.Sequential([\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(2,2)),\n",
    "#             M.UpSampling2D((2, 2)),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(3,3)),\n",
    "#             M.UpSampling2D((2, 2)),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(4,4)),\n",
    "#             M.UpSampling2D((1, 1)),\n",
    "#             M.Conv2D(32, (3, 3), padding='same', activation='relu',dilation_rate=(4,4)),\n",
    "#             M.UpSampling2D((1, 1)),\n",
    "#             M.Conv2D(1, (3, 3), padding='same', activation='sigmoid')\n",
    "#         ])\n",
    "#     def forward(self, inputs):\n",
    "#         e=self.encoder(inputs)\n",
    "#         y=self.decoder(e)\n",
    "#         return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a8502-a7da-416a-b112-210103492dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查参数量\n",
    "# model = Predictor()\n",
    "# # print(model)\n",
    "# autoencoder=Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f27c3e-a046-4aa0-86cc-91e5dc663479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from megengine.utils.module_stats import module_stats\n",
    "\n",
    "# input_data = np.random.rand(1, 1, 256, 256).astype(\"float32\")\n",
    "# total_stats, stats_details = module_stats(\n",
    "#     net,\n",
    "#     inputs = (input_data,),\n",
    "#     cal_params = True,\n",
    "#     cal_flops = True,\n",
    "#     logging_to_stdout = True,\n",
    "# )\n",
    "\n",
    "# print(\"params %.3fK MAC/pixel %.0f\"%(total_stats.param_dims/1e3, total_stats.flops/input_data.shape[2]/input_data.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813a5fb-f1c5-41d1-ad66-b457d60ee28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da5d47-0e1f-4d02-b4f0-82daaf9389f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
